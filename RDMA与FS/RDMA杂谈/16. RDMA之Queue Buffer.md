# 背景
RDMA中的Queue是一个循环队列，经常称为Ring Buffer：
![[Pasted image 20240121101439.png]]

在物理内存中，由于内存是线性的，所以我们依靠头尾指针的维护来实现一个循环队列。

# 面临的问题
1. 硬件访问内存用的是DMA地址，即IOVA或PA，并不是用户看到的VA，两者不在同一个地址空间。也就是说，硬件不能拿着CPU使用的虚拟地址去访问内存。如下图所示：
![[Pasted image 20240121101729.png]]

2. 操作系统的换页功能会导致虚拟地址与物理地址的映射关系变化。对于CPU而言，虚拟地址不变，物理地址变化，但对于硬件而言，之前拥有的DMA地址就会失效。

3. 虚拟地址连续，物理地址不一定连续。就算我们可以把内存的DMA地址告诉硬件，也要把这些离散的页的地址都告诉硬件。

由于操作系统权限问题，上述几个问题都必须在内核态中进行解决，这也是Create QP等控制路径的Verbs不得不陷入内核态的原因之一。对于RDMA设备而言，需要解决的问题如下：
1. 获取物理内存页的DMA地址。
2. 为虚拟页面映射物理页，并固定虚拟页和物理页间的关系。
3. 以硬件可以识别的方式组织物理内存页的DMA地址。

# 创建流程（控制路径）
## 用户态
### 用户调用创建QP的API
用户首先在用户程序中调用Create QP的Verbs API来启动创建QP的流程

### 驱动申请Buffer
用户驱动程序会根据用户指定的Queue的深度和硬件WQE大小，申请一片虚拟内存。

用户态申请完虚拟内存，以及做一些其他软件资源的初始化之后，将通过软件框架封装的系统调用接口陷入内核态。

## 内核态
### 获取Buffer的信息
在RDMA用户态驱动通过系统调用陷入到内核态时，会传递Buffer大小相关的信息以及起始虚拟地址，内核驱动会暂时记录这两个信息用于稍后使用。

### 为虚拟页面映射物理页，并固定虚拟页和物理页间的关系
利用Pin机制，将物理页固定在page buffer中，从而固定虚拟也和物理页间的关系

### 获取物理页的DMA地址
系统提供了DMA地址映射接口，来获取物理内存页的DMA地址。如果系统支持IOMMU/SMMU，那么这里获取到的是IOVA，如果不支持，得到的就是PA。

经过以上步骤，我们就可以获取以下列表，每个scatterlist结构中，都存放了其指向的Buffer的虚拟地址、长度、DMA地址、所在的物理页以及页内偏移等信息。多个scatterlist放在一起，就可以描述一组离散的Buffer了，我们称其为SG Table。
![[Pasted image 20240121103033.png]]

### 以硬件可以识别的方式组织物理内存页的DMA地址
由于SG Table是一个CPU视角的数据结构，操作系统内核也是通过VA来访问和管理这个数组的，其本身的物理地址未必连续，因此没法直接给RDMA网卡使用。

所以这里需要驱动程序**另外申请**一片物理地址连续的Buffer，用来拷贝SG Table的所有信息，最后将这个新申请的用于存放Buffer信息的DMA首地址填入到QPC中。
![[Pasted image 20240121103229.png]]

# 使用流程（数据路径）
## 用户态
### 用户通过API下发WR
用户的应用程序通过rdma-core提供的Verbs接口来Post Send一个WR，WR中包含了要发送给对端的请求的信息

### 驱动将WR转化为WQE并填入QP Buffer
各厂商的用户态驱动收到用户的WR后，按照格式配置硬件相关的WQE的内容

### 驱动敲Doorbell
填写完WQE之后，驱动要想办法通知硬件，而这个通知机制就是Doorbell。

## 硬件
### 解析Doorbell
硬件收到Doorbell之后，会解析其内容，拿到前文说的QPN和头指针。

### 从QPC中获取QP Buffer信息
为什么Doorbell中要携带QPN呢，因为硬件需要通过QPN找到对应的QPC。

### 获取并解析WQE
硬件通过上一步从QPC中获取到的QP Buffer的信息（Buffer Table的基地址，每一个物理Buffer的长度等），加上Doorbell中的头指针，就可以算出新下发的WQE的DMA地址，从而取出WQE并进行解析。

# 总结
![[Pasted image 20240121104059.png]]
控制路径需要经过内核，而数据路径绕开了内核。